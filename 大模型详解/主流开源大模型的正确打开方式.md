
最近开源大模型迭代速度真的是越来越快了，特别是META最近又开源了新的LLAMA2模型，使得开源社会蓬勃发展 。

因此本文主要总结了几个主流的开源大模型，并简单分享自己的一些使用体验，帮助各位初学者更快的使用这些大模型。

| **模型** | **项目地址** | **标签** |
| --- | --- | --- |
| LLAMA-2 | [https://github.com/facebookresearch/llama](https://github.com/facebookresearch/llama) | Meta开源大模型 |
| ChatGLM2-6B | [https://github.com/THUDM/ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) | 更适合中文的问答模型 |
| Llama2-Chinese | [https://github.com/FlagAlpha/Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese) | 中文Llama大模型 |
| Chinese-LLaMA-Alpaca | [https://github.com/ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | 指令理解（问答、写作、建议等）；多轮上下文理解（聊天等） |

# LLAMA-2
![](https://cdn.nlark.com/yuque/0/2023/webp/29330410/1690379957243-599de663-e948-4c5b-a099-8687356b8c2d.webp#averageHue=%232c8c98&clientId=u16bfbe71-743e-4&from=paste&id=u498d745e&originHeight=475&originWidth=847&originalType=url&ratio=1.3499999046325684&rotation=0&showTitle=false&status=done&style=none&taskId=ufe139de7-4063-4a1f-b0e7-c66a0088670&title=)
## 1.模型能力
要说开源模型比较出名的，当然是Meta最近发布的二代Llama模型。其目前开源了三种不同的参数模型：7B，13B，70B
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690250731924-beae412d-fb76-451e-b248-c963f00b7473.png#averageHue=%23d9dde2&clientId=u16bfbe71-743e-4&from=paste&height=496&id=u54dc72ba&originHeight=670&originWidth=1821&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=142198&status=done&style=none&taskId=ue238caed-9b82-48ca-8d83-01fd44c6e46&title=&width=1348.8889841778355)
### 训练方法和能力提升
LLAMA2中，相比于LLAMA1主要引入了RLHF（**人类反馈强化学习，也就是在训练ChatGPT提到的一个技术**）。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690270507553-a3fd469f-fcbf-4325-82e1-a7396d867693.png#averageHue=%23f9fbfa&clientId=u16bfbe71-743e-4&from=paste&height=539&id=u3d5b04b4&originHeight=727&originWidth=1428&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=118714&status=done&style=none&taskId=uc2bf5814-3bb3-4d3f-bef1-8a87c075c4e&title=&width=1057.7778525018941)
训练 Llama-2-chat：Llama 2 使用公开的在线数据进行预训练。然后通过使用监督微调创建 Llama-2-chat 的初始版本。 它使用人类反馈强化学习 (RLHF) 进行迭代细化，其中包括拒绝采样和近端策略优化 (PPO)。

从论文中来看，其能力相比于一代有明显的提升，在代码能力、数学能力、上下文理解等有大幅度提升：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690376196705-094b61d0-ed5f-4b6b-ac4b-5e6477ec2b02.png#averageHue=%23ededed&clientId=u16bfbe71-743e-4&from=paste&height=291&id=u2e286440&originHeight=275&originWidth=680&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=104487&status=done&style=none&taskId=u8dc7f262-d69c-480c-bdad-2c9b02519b1&title=&width=718.4444580078125)

### 安全性
通常来说，有用性和安全性是模型的两个不同矛盾点。也就是这两种能力会互相抵消。为了解决这个问题，在LLAMA2中使用了两种不同的奖励模型：**其一针对有用性进行优化（Helpativity RM），其二针对安全性进行优化（Safety RM）。**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690376507928-df732ddc-e3e7-416e-851c-74d9bb1a14c0.png#averageHue=%23ededed&clientId=u16bfbe71-743e-4&from=paste&id=u19b26f7b&originHeight=600&originWidth=794&originalType=url&ratio=1.3499999046325684&rotation=0&showTitle=false&size=79282&status=done&style=none&taskId=uc6d059d5-3110-4c9a-a01a-fe8336d2cae&title=)
上表比较了 Llama 2 与 Llama 1、Falcon 和 MPT 的性能差异。与 Llama 7B 模型相比，Llama 2-7B 的真实性和信息性提高了 21.37%，有毒内容比例降低了 7.61%。

## 2.模型申请下载
填写对应的名称，然后点击同意就可以申请，申请网址：[https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1689824703915-aeaaf8a0-a2e8-4f76-8bde-7e61946521ac.png#averageHue=%23fefefe&clientId=u635d39a9-bac4-4&from=paste&height=646&id=u72361f7f&originHeight=872&originWidth=2184&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=57427&status=done&style=none&taskId=u8682b0a6-1129-4127-ac1b-6e450c44205&title=&width=1617.7778920617204)

如果需要在huggingface下载hf模型，同时需要去到huggingface上申请。

审核很快就通过，我在下午申请，晚上就收到邮件通知说通过了
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1689824832018-4876e622-90da-48f7-857f-a900d3202369.png#averageHue=%23fefdfd&clientId=u635d39a9-bac4-4&from=paste&height=393&id=u06ebeda7&originHeight=531&originWidth=1774&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=69886&status=done&style=none&taskId=u7feb866e-0c73-49c9-a336-c894ca54628&title=&width=1314.0741669036136)

之后就可以直接去huggingface网站下载了：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1689840407185-a855af77-307e-4163-b1e7-221b7eeef13b.png#averageHue=%23e7ccaa&clientId=u635d39a9-bac4-4&from=paste&height=610&id=ucc6f373b&originHeight=823&originWidth=1872&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=160936&status=done&style=none&taskId=ufe9f94a9-8eba-43f6-a547-165f81b3e99&title=&width=1386.6667646243318)

## 3.模型调用与微调
项目地址：[https://github.com/facebookresearch/llama](https://github.com/facebookresearch/llama)

参考仓库中的`example_chat_completion.py`，把对应的数据构造成合适的格式就可以微调
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690379386770-926bd491-3689-450b-9bf0-1bf540398167.png#averageHue=%2310151c&clientId=u16bfbe71-743e-4&from=paste&height=658&id=ued3de36a&originHeight=740&originWidth=1609&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=68943&status=done&style=none&taskId=ub50db0b0-4278-4253-a8b9-eb688e24676&title=&width=1430.2222222222222)
```python
torchrun --nproc_per_node 1 example_chat_completion.py \
    --ckpt_dir llama-2-7b-chat/ \
    --tokenizer_path tokenizer.model \
    --max_seq_len 512 --max_batch_size 4
```

## 4.回答体验
如果你输入中文给他，他虽然能够理解你的意思，但是还是很固执的回复你英文。猜测是因为训练集中只有少量的中文导致。

**比如我输入：什么工作钱多事少离家近？**它没有回复我具体内容，只是说：少工作”和“赚更多”的想法并不总是一个现实或可持续的目标，因为它可能对你的身心健康有害，也可能导致倦怠。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690381306767-5552a75a-ef33-42f9-9eb1-90c307e52d7e.png#averageHue=%23ebefe6&clientId=u16bfbe71-743e-4&from=paste&height=468&id=u465497fd&originHeight=526&originWidth=1556&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=101709&status=done&style=none&taskId=ud8bc09b8-1d38-4204-97b9-a13deb349a8&title=&width=1383.111111111111)
好像还说得有点道理。

**输入：A\B\C\D共四个人的距离，让他计算A离它最近的是哪个人。它也能够正确输入，且能够有效识别markdown格式。**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690381489666-77b33b28-567b-4b05-a31a-4a6380de5f8f.png#averageHue=%23e2f0e5&clientId=u16bfbe71-743e-4&from=paste&height=466&id=u3fb1b4bf&originHeight=524&originWidth=943&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=66676&status=done&style=none&taskId=u76abef43-167a-4b22-bd3e-11a4864ea33&title=&width=838.2222222222222)

总体来说，相比于一代确实有明显的提升。

# ChatGLM2-6B
目前在开源大模型中，比较有名的是Meta的LLAMA模型系列和清华的ChatGLM模型。其中特别是在中文领域上，ChatGLM模型经过中文问答和对话的优化，更加符合中文使用者的偏好回答。
## 1.模型能力

经过一段时间的迭代，ChatGLM模型引来了重大更新。新推出的2代版本叫ChatGLM2模型。它保留了初代模型的优秀特性，并引入了新的优点。

**首先其表现出更加强大的性能：**全面超越第一代模型，在多个数据集上有大幅度的提升，比初代模型在MMLU（+23%）、CEval（+33%）、GSM8K（+571%）、BBH（+60%）等方面具有更强的竞争力。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1688213508228-88edb61e-7e8c-48c3-85ea-f25b3d9e4503.png#averageHue=%23fefdfc&clientId=ufedaf2c9-cc65-4&from=paste&height=507&id=u59139a91&originHeight=570&originWidth=1072&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=69236&status=done&style=none&taskId=u88cd9cf5-2f69-4d54-a3c3-0e10de9831c&title=&width=952.8888888888889)

**同时可以处理更长的文本**：第一代模型只能处理2K，到第二代模型扩展到了32K，同时还可以进行多轮次对话

**另一个优点可以节省推理空间：**利用Multi-Query Attention技术，ChatGLM2-6B具有更高效的推理速度和更低的显存占用。

在2048长度的文本输入下，其推理速度比ChaGLM第一代模型快13秒
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1688213736045-6c207ce6-b6a6-414e-849e-3f26852e8de8.png#averageHue=%23e0be8d&clientId=ufedaf2c9-cc65-4&from=paste&height=145&id=u8692ae06&originHeight=163&originWidth=388&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=13047&status=done&style=none&taskId=u2adfd31f-cb44-45c8-bbd5-29e1a3e5443&title=&width=344.8888888888889)

该项目目前已经有6.6K星：[https://github.com/THUDM](https://github.com/THUDM)
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1688213462551-37524260-db1b-4d05-9fec-9f546c87ec88.png#averageHue=%230f141b&clientId=ufedaf2c9-cc65-4&from=paste&height=165&id=ua709e504&originHeight=186&originWidth=584&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=12578&status=done&style=none&taskId=u9b643d0d-d60a-44a8-8c8d-b9cf8aeef88&title=&width=519.1111111111111)

## 2.使用方法
如果要使用该模型，可以先下载github的仓库代码，然后安装对应的依赖包
```python
git clone https://github.com/THUDM/ChatGLM2-6B
cd ChatGLM2-6B
pip install -r requirements.txt
```

然后运行代码就可以了：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1688217491301-e095cc61-e4d3-4737-98e6-1a6be3722deb.png#averageHue=%23e7eaed&clientId=ufedaf2c9-cc65-4&from=paste&height=429&id=uf20c62f5&originHeight=483&originWidth=1031&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=59889&status=done&style=none&taskId=u0c34aa72-d5b3-4bb9-a663-f5ef3a0790a&title=&width=916.4444444444445)

## 3.对比GPT4（必应）
### Q1：什么工作钱多事少离家近？


**ChatGLM2：技术岗位、销售岗位、管理岗位、创意岗位**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1688217805316-80c9032f-f485-4857-b736-c1b98b7a6c24.png#averageHue=%23f6f1cb&clientId=ufedaf2c9-cc65-4&from=paste&height=530&id=u47fff549&originHeight=596&originWidth=1529&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=109077&status=done&style=none&taskId=uf0f12987-93bb-48a2-a5e0-c762b8185a1&title=&width=1359.111111111111)


**Bing：保险代理人、程序员、翻译、市场营销**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1684815395807-f1014338-c766-4e74-82e8-4b6941686886.png#averageHue=%23e2e8f7&clientId=u2e2fdfcf-8dd3-4&from=paste&height=626&id=u9c880d3d&originHeight=751&originWidth=893&originalType=binary&ratio=1.2000000476837158&rotation=0&showTitle=false&size=234660&status=done&style=none&taskId=u28bd7cee-b785-46f8-8cfa-345a6514b59&title=&width=744.1666370961412)

### Q2：一个三角形,如果一条边长为4cm,另一条边长为7cm,则第三条边最长可能是多少厘米?
## 
**ChatGLM2：10cm（答案正确）**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1688217874306-171ead3d-aa46-4fbf-9231-72e26785307d.png#averageHue=%23f6f9dc&clientId=ufedaf2c9-cc65-4&from=paste&height=516&id=u22429bb0&originHeight=581&originWidth=1547&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=71234&status=done&style=none&taskId=u16f27b28-0467-47f5-9d2e-7264a1faa6b&title=&width=1375.111111111111)

**Bing：10cm（答案正确）**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1684835707107-e069c690-c754-43e8-9696-f14cf1ebbc0e.png#averageHue=%23c9d3f5&clientId=ubdcac386-b027-4&from=paste&height=364&id=uc5ca19cb&originHeight=437&originWidth=913&originalType=binary&ratio=1.2000000476837158&rotation=0&showTitle=false&size=131168&status=done&style=none&taskId=u63d036b6-4de2-4ebc-9013-1a41b22d7c1&title=&width=760.8333031005342)

### Q3：用python写一个斐波拉契函数
**ChatGLM2：**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1688217961925-e1efddbb-5bd3-48e0-acc1-6af460e7d34e.png#averageHue=%23ccdc9e&clientId=ufedaf2c9-cc65-4&from=paste&height=516&id=ua16490ed&originHeight=581&originWidth=1552&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=68033&status=done&style=none&taskId=ub69d0dc4-f973-44fb-9acf-29241f40585&title=&width=1379.5555555555557)


**Bing：**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1684843215756-2f58bd66-f1b3-4650-853d-c70925ca898e.png#averageHue=%23e6eaf7&clientId=ubdcac386-b027-4&from=paste&height=739&id=u2c211346&originHeight=887&originWidth=914&originalType=binary&ratio=1.2000000476837158&rotation=0&showTitle=false&size=194218&status=done&style=none&taskId=ua65ee0fb-dc51-4e00-9989-a053868de4b&title=&width=761.6666364007538)

从结果上看，ChatGLM2和GPT4的结果答案很相似，说明整体的模型效果确实能够不断接近GPT4了。

# Llama2-Chinese
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690381666428-f8ced0d6-4777-48c3-846a-87716ed286a2.png#averageHue=%23fdfaf9&clientId=u16bfbe71-743e-4&from=paste&height=416&id=u027116f6&originHeight=468&originWidth=1096&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=68210&status=done&style=none&taskId=u5c5bdec7-89a1-4c3b-8723-84f04013ba9&title=&width=974.2222222222222)

该项目在LLAMA2开源几天，就已经放出了，速度之快简直令人惊奇。另外作者还提供在线网站进行体验：
[https://llama.family/](https://llama.family/)

该项目主要是为了让LLAMA2在中文领域上回答得更加顺滑。

## 1.模型数据集和微调
从官方放出的资料来看，微调的数据集主要来源于网络数据、维基百科、悟道等数据。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690382018937-cbfaf096-e8bf-42b6-ae3b-3701688e84c7.png#averageHue=%23f5e4b5&clientId=u16bfbe71-743e-4&from=paste&height=362&id=ua0f22a63&originHeight=407&originWidth=1060&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=49091&status=done&style=none&taskId=u897fb94a-1134-46ba-aae4-d6ff5432714&title=&width=942.2222222222222)

在微调过程中，也可以直接把数据集做成下面格式：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690382181127-0943cbd0-d12b-4dee-a62d-978713abdf80.png#averageHue=%23f2f4f7&clientId=u16bfbe71-743e-4&from=paste&height=46&id=uf8e06ea8&originHeight=52&originWidth=1025&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=2755&status=done&style=none&taskId=u901f06b7-2212-4897-9a7e-4bc00e7ef80&title=&width=911.1111111111111)

然后调用该仓库的代码就可以了`train/sft/finetune.sh`

## 2.回答体验
### Q1: 代码生成
在代码生成方面，一般的问题还是难不倒它，而且相比于原版，中文LLAMA模型还是知道用中文回复你
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690383996171-889a5905-24b6-40e0-a58c-698d3f9618bc.png#averageHue=%23e1eec5&clientId=u16bfbe71-743e-4&from=paste&height=590&id=u4f599d31&originHeight=664&originWidth=1978&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=73728&status=done&style=none&taskId=u88c969b2-9ec2-4166-b29d-e0a1d082767&title=&width=1758.2222222222222)

### Q2：语言理解
**输入：**对以下两篇文章进行比较，确定它们是否讨论了相同的话题。输入两篇文章的文本内容，输出是“是”或“否”。 文章1：Python是一种解释型、面向对象、动态数据类型的高级程序设计语言。Python由Guido van Rossum创建，第一个公开发行版发行于1991年。目前，Python是一种流行的编程语言，被广泛应用于Web开发、科学计算、人工智能等领域。 文章2：Java是一种高级程序设计语言和计算机平台。它的设计最初是由James Gosling在Sun Microsystems进行的。自从2009年Oracle收购了Sun以来，Java就成为Oracle的产品。Java可以在不同的计算机平台上编写一次程序，然后在这些平台上运行多次。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690386588907-b69df792-ab50-46ce-82a7-45e35f0e0590.png#averageHue=%23f5f6d9&clientId=u16bfbe71-743e-4&from=paste&height=254&id=u6d58bc84&originHeight=286&originWidth=2001&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=60686&status=done&style=none&taskId=ua3cd3569-4cec-4ecf-b012-31f5cb1bdc8&title=&width=1778.6666666666667)

输出结果：不是相同主题，因为是两个不一样的编程语言

### Q3：角色扮演
 输入：请描述一下作为一名AI语音助手的日常工作流程是如何进行的
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690457838070-d78aded3-1d3e-4b70-8667-9486a1b86ca4.png#averageHue=%23e4c18f&clientId=u16bfbe71-743e-4&from=paste&height=391&id=u6d4bdf0e&originHeight=528&originWidth=1939&originalType=binary&ratio=1.3499999046325684&rotation=0&showTitle=false&size=118266&status=done&style=none&taskId=ud7ab842d-0873-4e67-8933-57f907b7e46&title=&width=1436.2963977599247)
可以有效扮演你给他设定的角色，回答得是比较有条理性得。

# Chinese-LLaMA-Alpaca
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690639716837-2ca6bd5a-6715-47b6-b58c-f22bc498959b.png#averageHue=%23eae8e7&clientId=u062ba76a-746c-4&from=paste&height=237&id=u4d42b740&originHeight=267&originWidth=952&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=116677&status=done&style=none&taskId=ub41c8f7a-5256-4554-bac9-d4363a17ff3&title=&width=846.2222222222222)
这个项目主要是基于开源的Alpaca大模型进行指令微调。**在原版LLaMA的基础上扩充了中文词表**并使用了中文数据进行二次预训练，进一步提升了中文基础语义理解能力。同时，中文Alpaca模型进一步使用了中文指令数据进行精调，显著提升了模型对指令的理解和执行能力。

而且在本地部署后实际的推理效果还是相当可以的：
![](https://github.com/ymcui/Chinese-LLaMA-Alpaca/raw/main/pics/screencast.gif#from=url&id=OIeai&originHeight=666&originWidth=1011&originalType=binary&ratio=1.125&rotation=0&showTitle=false&status=done&style=none&title=)

## 1.模型下载和合成
该项目主要是利用Lora模型进行训练，所以除了要下载原版的LLAMA模型之外，还需要下载它的Lora参数模型。

并把两个模型参数进行合并：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690639911311-3a4a523c-88db-4bbd-b7c0-6094511f318e.png#averageHue=%23f6f1c9&clientId=u062ba76a-746c-4&from=paste&height=130&id=ud0df77ef&originHeight=146&originWidth=736&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=21845&status=done&style=none&taskId=u5a840823-36be-4014-9cb6-cae0b400179&title=&width=654.2222222222222)

手动转换教程可以参考这里：[https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E6%89%8B%E5%8A%A8%E6%A8%A1%E5%9E%8B%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BD%AC%E6%8D%A2)
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690640000539-36c581c6-d70e-4f8a-83ff-14fe3d7f39c3.png#averageHue=%23c8eccc&clientId=u062ba76a-746c-4&from=paste&height=254&id=u03053210&originHeight=286&originWidth=1142&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=28733&status=done&style=none&taskId=u1e4e2afe-5b52-469b-8855-e1b8c426871&title=&width=1015.1111111111111)
## 2.回答体验
数学能力感觉没有达到相应的水平，回答是错误的：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690640175656-94ff1933-0047-4563-89d1-6b6e436536c0.png#averageHue=%23fef4e8&clientId=u062ba76a-746c-4&from=paste&height=250&id=u35747c07&originHeight=281&originWidth=925&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=16435&status=done&style=none&taskId=u04ab5702-ef63-430b-a58b-7f6fb9a6fec&title=&width=822.2222222222222)

对于这种论证题目，给出的论证和论点都比较充分：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1690640235761-a2d9ccda-6036-469d-99c2-616fcd8f0009.png#averageHue=%23d7f9f8&clientId=u062ba76a-746c-4&from=paste&height=598&id=u88a1fa4d&originHeight=673&originWidth=900&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=83659&status=done&style=none&taskId=u39f9de47-ee9c-4f35-be5c-3cd63ca0129&title=&width=800)

从整体的测试来看，除了数学能力一般之外，其他的能力还是初步具备了。

针对大模型，越来越多的公司已经开源了。虽然相比于OpenAI的ChatGPT和GPT4模型效果还是有一定的距离，但是目前测试来看，LLAMA2模型已经无限接近于GPT3.5的效果。假以时日，开源大模型说不定真的能达到GPT4的效果。

而且开源大模型也让普通玩家能够训练自己特有的领域模型，更加适应不同领域业务，反过来也会进一步推动大模型的蓬勃发展。

好了，以上就是本期的所有内容了，我是leo，我们下期再见~
![qrcode_for_gh_f4f620aeff8d_258.jpg](https://cdn.nlark.com/yuque/0/2023/jpeg/29330410/1690640566323-fb77356d-0487-40e2-971d-0a8899429a98.jpeg#averageHue=%23a6a4a3&clientId=u062ba76a-746c-4&from=paste&height=229&id=u4386c32f&originHeight=258&originWidth=258&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=27597&status=done&style=none&taskId=ub479ffec-ea64-4058-b5ab-efd6bcc7332&title=&width=229.33333333333334)
