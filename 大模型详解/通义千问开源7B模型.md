之前我曾经测评过几个比较主流的7B模型，包括LLAMA2、ChatGLM2和Alpaca模型，详情可以看回我之前的文章。
[主流开源大模型的正确打开方式](https://www.yuque.com/yuqueyonghumaryyq/fmvho1/cilvc2v5sfew9beg?view=doc_embed)

| **模型** | **项目地址** | **标签** |
| --- | --- | --- |
| LLAMA-2 | [https://github.com/facebookresearch/llama](https://github.com/facebookresearch/llama) | Meta开源大模型 |
| ChatGLM2-6B | [https://github.com/THUDM/ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) | 更适合中文的问答模型 |
| Llama2-Chinese | [https://github.com/FlagAlpha/Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese) | 中文Llama大模型 |
| Chinese-LLaMA-Alpaca | [https://github.com/ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | 指令理解（问答、写作、建议等）；多轮上下文理解（聊天等） |


这不，阿里终于把通义千问进行开源了，可以在这个仓库上看到：[https://github.com/QwenLM/Qwen-7B](https://github.com/QwenLM/Qwen-7B)
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1691327712957-d3ac9bc5-1666-418c-88ea-c3c2c28fb5e2.png#averageHue=%23f4f4f3&clientId=u787b190d-dd6f-4&from=paste&height=312&id=u8e83ebba&originHeight=351&originWidth=705&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=63266&status=done&style=none&taskId=u8028082e-6ffa-4ee6-8fbf-3eeb65ac0a0&title=&width=626.6666666666666)

从官网中介绍，通义千问有以下几个优点：

- **训练时使用了大规模的高质量数据：**使用了超过2.2万亿token进行预训练
- **更好地支持多语言**：基于更大词表的分词器在分词上更高效，同时它对其他语言表现更加友好。用户可以在Qwen-7B的基础上更方便地训练特定语言的7B语言模型。
- **支持8K长度上下文：**允许用户输入更长的prompt。
- **评测能力有大幅提升：**通义千问在多个评测数据集上具有显著优势，甚至超出12-13B等更大规模的模型。

从实验中看出，通义千问模型在多个数据集评测上都超过现有的开源模型，而且甚至比之前META开源的LLAMA2-7B模型效果要好：
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1691327878397-f6ec96c9-33a2-43db-9028-f2b80422be0a.png#averageHue=%23f9eae1&clientId=u787b190d-dd6f-4&from=paste&height=527&id=u2ab5c206&originHeight=593&originWidth=1299&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=77466&status=done&style=none&taskId=uaa66f503-45e8-4782-9d0c-31ca84b997f&title=&width=1154.6666666666667)

# 模型测评（对比GPT4）
代码调用中，只需要通过使用huggingface就可以搭建推理模型：
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# 请注意：分词器默认行为已更改为默认关闭特殊token攻击防护。相关使用指引，请见examples/tokenizer_showcase.ipynb
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# 打开bf16精度，A100、H100、RTX3060、RTX3070等显卡建议启用以节省显存
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# 打开fp16精度，V100、P100、T4等显卡建议启用以节省显存
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# 使用CPU进行推理，需要约32GB内存
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# 默认使用自动模式，根据设备自动选择精度
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()

# 可指定不同的生成长度、top_p等相关超参
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# 第一轮对话 1st dialogue turn
response, history = model.chat(tokenizer, "你好", history=None)
print(response)
# 你好！很高兴为你提供帮助。
```

### Q1：什么工作钱多事少离家近？


**通义千问：教师、护士、零售员工、客服。**在这个回答中，前面两个答案还可以，但是后面的答案像是随机生成的，并不符合我的提问要求。
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1691328217431-1dded366-61eb-4bd4-98a4-e36df12cea67.png#averageHue=%23f6f7f7&clientId=u787b190d-dd6f-4&from=paste&height=512&id=ub3a6e0e6&originHeight=576&originWidth=1272&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=93203&status=done&style=none&taskId=uca4e43ca-5715-4e66-b620-8b2d9d96fc1&title=&width=1130.6666666666667)


**Bing：保险代理人、程序员、翻译、市场营销**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1684815395807-f1014338-c766-4e74-82e8-4b6941686886.png#averageHue=%23e2e8f7&clientId=u2e2fdfcf-8dd3-4&from=paste&height=626&id=u9c880d3d&originHeight=751&originWidth=893&originalType=binary&ratio=1.2000000476837158&rotation=0&showTitle=false&size=234660&status=done&style=none&taskId=u28bd7cee-b785-46f8-8cfa-345a6514b59&title=&width=744.1666370961412)

### Q2：一个三角形,如果一条边长为4cm,另一条边长为7cm,则第三条边最长可能是多少厘米?
## 
**通义千问：11cm（答案错误），简单的数学能力不太行**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1691328343737-a60d3f0d-2786-4c2a-bf4d-84b013914d94.png#averageHue=%23f9f8fb&clientId=u787b190d-dd6f-4&from=paste&height=327&id=u635fb271&originHeight=368&originWidth=1265&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=29247&status=done&style=none&taskId=u984bd266-4c55-4bd9-9e2a-ed0ccd98053&title=&width=1124.4444444444443)

**Bing：10cm（答案正确）**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1684835707107-e069c690-c754-43e8-9696-f14cf1ebbc0e.png#averageHue=%23c9d3f5&clientId=ubdcac386-b027-4&from=paste&height=364&id=uc5ca19cb&originHeight=437&originWidth=913&originalType=binary&ratio=1.2000000476837158&rotation=0&showTitle=false&size=131168&status=done&style=none&taskId=u63d036b6-4de2-4ebc-9013-1a41b22d7c1&title=&width=760.8333031005342)

### Q3：用python写一个斐波拉契函数
**通义千问：可以正确的写出代码**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1691329249636-c06ef6ca-f705-4c75-bd5e-795e131611f7.png#averageHue=%23f1f2f4&clientId=u787b190d-dd6f-4&from=paste&height=460&id=ufb4004fb&originHeight=518&originWidth=1270&originalType=binary&ratio=1.125&rotation=0&showTitle=false&size=54212&status=done&style=none&taskId=u80685fa1-00c7-4991-a38e-b3d2677e311&title=&width=1128.888888888889)


**Bing：可以正确的写出代码**
![image.png](https://cdn.nlark.com/yuque/0/2023/png/29330410/1684843215756-2f58bd66-f1b3-4650-853d-c70925ca898e.png#averageHue=%23e6eaf7&clientId=ubdcac386-b027-4&from=paste&height=739&id=u2c211346&originHeight=887&originWidth=914&originalType=binary&ratio=1.2000000476837158&rotation=0&showTitle=false&size=194218&status=done&style=none&taskId=ua65ee0fb-dc51-4e00-9989-a053868de4b&title=&width=761.6666364007538)

从结果上看，在普通的问答环节上，可以简单的理解用户的需求，但是数学能力比较欠缺，简单的数学题目也做不太对。

# 总结
就像很多网友所说的一样，尽管通义千问开源了，但是仅仅开源7B模型还是不能够掀起多大的浪花出来，究其原因还是在于：

- **模型参数太少：**现有的7B左右参数的模型已经太卷了，对比于动辄70B以上的模型来说，7B模型还是太小了
- **单独刷榜单没有什么效果：**利用现有的榜单进行刷榜，其实人们已经很少去关注了，刷榜只会给人们带来麻木
- 在开源模型中，目前只有LLAMA2真正使用了RLHF，其他模型还是优点欠缺

好了，以上就是本期的所有内容了，我是leo，我们下期再见~

